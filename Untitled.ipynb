{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', '.ipynb_checkpoints', 'data_text.txt', 'pip-requirements.txt', 'prepare_data.py', 'raw_data', 'segment.sh', 'segment_test.txt', 'stanford-segmenter-2017-06-09', 'train_log', 'Untitled.ipynb', 'word2vec.py']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "logging.basicConfig(format=' %(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "ls = os.listdir(cwd)\n",
    "\n",
    "print(ls)\n",
    "\n",
    "f = open(\"data_text.txt\", 'r+')\n",
    "\n",
    "# line = ''\n",
    "# for i in range(10):\n",
    "# \tline = f.readline()\n",
    "# \tprint(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(line):\n",
    "    sentence = []\n",
    "    str = ''\n",
    "    for char in line:\n",
    "        if ord(char) != ord(' '):\n",
    "            str += char\n",
    "        else:\n",
    "            if str != '':\n",
    "                sentence.append(str)\n",
    "            str = ''\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete!\n",
      "Num of sentences: 14001 \n"
     ]
    }
   ],
   "source": [
    "line = ''\n",
    "sentences = []\n",
    "while 1:\n",
    "    line = f.readline()\n",
    "    s = sentence_to_wordlist(line)\n",
    "#     print(len(s))\n",
    "    sentences.append(s)\n",
    "    \n",
    "    if len(line) == 0:\n",
    "        print(\"Complete!\")\n",
    "        break\n",
    "    \n",
    "print ('Num of sentences: {} '.format(len(sentences)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus contains 196,028,001 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum(len(sentences) for s in sentences)\n",
    "print('Corpus contains {0:,} tokens'.format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 2000\n",
    "\n",
    "min_word_count = 3\n",
    "\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "context_size = 7\n",
    "\n",
    "downsampling = 1e-3\n",
    "\n",
    "seed = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "People_Daily_2_Vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2018-08-16 17:41:34,328 : INFO : collecting all words and their counts\n",
      " 2018-08-16 17:41:34,329 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      " 2018-08-16 17:41:36,163 : INFO : PROGRESS: at sentence #10000, processed 4973825 words, keeping 115633 word types\n",
      " 2018-08-16 17:41:37,160 : INFO : collected 155857 word types from a corpus of 7590112 raw words and 14001 sentences\n",
      " 2018-08-16 17:41:37,162 : INFO : Loading a fresh vocabulary\n",
      " 2018-08-16 17:41:37,724 : INFO : effective_min_count=3 retains 65340 unique words (41% of original 155857, drops 90517)\n",
      " 2018-08-16 17:41:37,725 : INFO : effective_min_count=3 leaves 7469420 word corpus (98% of original 7590112, drops 120692)\n",
      " 2018-08-16 17:41:37,937 : INFO : deleting the raw counts dictionary of 155857 items\n",
      " 2018-08-16 17:41:37,944 : INFO : sample=0.001 downsamples 28 most-common words\n",
      " 2018-08-16 17:41:37,945 : INFO : downsampling leaves estimated 6785561 word corpus (90.8% of prior 7469420)\n",
      " 2018-08-16 17:41:38,262 : INFO : estimated required memory for 65340 words and 2000 dimensions: 1078110000 bytes\n",
      " 2018-08-16 17:41:38,263 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "People_Daily_2_Vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length:  65340\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Vocab length: ', len(People_Daily_2_Vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# People_Daily_2_Vec.train(sentences, \n",
    "#                          total_examples=People_Daily_2_Vec.corpus_count,\n",
    "#                          epochs=People_Daily_2_Vec.epochs)\n",
    "\n",
    "if not os.path.exists(\"train_log\"):\n",
    "    os.makedirs(\"train_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2018-08-16 17:41:41,210 : INFO : saving Word2Vec object under train_log/People_Daily_2_Vec.w2v, separately None\n",
      " 2018-08-16 17:41:41,212 : INFO : storing np array 'vectors' to train_log/People_Daily_2_Vec.w2v.wv.vectors.npy\n",
      " 2018-08-16 17:41:42,109 : INFO : not storing attribute vectors_norm\n",
      " 2018-08-16 17:41:42,113 : INFO : storing np array 'syn1neg' to train_log/People_Daily_2_Vec.w2v.trainables.syn1neg.npy\n",
      " 2018-08-16 17:41:43,306 : INFO : not storing attribute cum_table\n",
      " 2018-08-16 17:41:43,480 : INFO : saved train_log/People_Daily_2_Vec.w2v\n"
     ]
    }
   ],
   "source": [
    "People_Daily_2_Vec.save(os.path.join(\"train_log\", \"People_Daily_2_Vec.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2018-08-16 17:41:43,486 : INFO : loading Word2Vec object from train_log/People_Daily_2_Vec.w2v\n",
      " 2018-08-16 17:41:43,895 : INFO : loading vocabulary recursively from train_log/People_Daily_2_Vec.w2v.vocabulary.* with mmap=None\n",
      " 2018-08-16 17:41:43,896 : INFO : loading wv recursively from train_log/People_Daily_2_Vec.w2v.wv.* with mmap=None\n",
      " 2018-08-16 17:41:43,897 : INFO : loading vectors from train_log/People_Daily_2_Vec.w2v.wv.vectors.npy with mmap=None\n",
      " 2018-08-16 17:41:44,258 : INFO : setting ignored attribute vectors_norm to None\n",
      " 2018-08-16 17:41:44,259 : INFO : loading trainables recursively from train_log/People_Daily_2_Vec.w2v.trainables.* with mmap=None\n",
      " 2018-08-16 17:41:44,260 : INFO : loading syn1neg from train_log/People_Daily_2_Vec.w2v.trainables.syn1neg.npy with mmap=None\n",
      " 2018-08-16 17:41:44,590 : INFO : setting ignored attribute cum_table to None\n",
      " 2018-08-16 17:41:44,591 : INFO : loaded train_log/People_Daily_2_Vec.w2v\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.13304811e-04, -7.28774758e-05, -1.26978985e-05, ...,\n",
       "        2.24579460e-04, -5.02919283e-05, -1.44902533e-05], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v = w2v.Word2Vec.load(os.path.join(\"train_log\", \"People_Daily_2_Vec.w2v\"))\n",
    "People_Daily_2_Vec.wv['的']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2018-08-16 17:41:44,785 : INFO : precomputing L2-norms of word weight vectors\n",
      "/anaconda3/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('方法论', 0.09949403256177902),\n",
       " ('越位', 0.09543273597955704),\n",
       " ('喝茶', 0.09023269265890121),\n",
       " ('老友', 0.08797562122344971),\n",
       " ('值班', 0.08685797452926636),\n",
       " ('丽敏', 0.08620989322662354),\n",
       " ('传控', 0.0856696143746376),\n",
       " ('竞拍', 0.08521401882171631),\n",
       " ('国泰航空', 0.08480346947908401),\n",
       " ('日记本', 0.08441604673862457)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "People_Daily_2_Vec.wv.most_similar(\"贱\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "People_Daily_2_Vec.wv.most_similar(\"可笑\")\n",
    "\n",
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.13304811e-04 -7.28774758e-05 -1.26978985e-05 ...  2.24579460e-04\n",
      " -5.02919283e-05 -1.44902533e-05]\n",
      "[-2.4995502e-04 -3.5159006e-05 -1.2827448e-04 ... -1.3422294e-04\n",
      " -2.3180127e-04 -1.0180235e-05]\n",
      "[ 1.2837346e-04 -9.6944488e-05  2.3100691e-04 ...  1.5547482e-04\n",
      " -7.1518662e-05  2.4443079e-04]\n",
      "[ 1.16237905e-04  9.24540873e-05 -1.10804231e-05 ... -1.92563297e-04\n",
      " -1.07979220e-04 -3.61402635e-05]\n",
      "[-8.7068380e-05 -2.0032641e-04  1.9706982e-04 ... -6.7635483e-05\n",
      "  6.9822954e-07  1.6898937e-04]\n",
      "[-1.1933129e-04  9.1349648e-05 -2.1032050e-04 ...  3.9540606e-05\n",
      " -1.5666206e-04  2.4368786e-04]\n",
      "[ 9.2808521e-05  1.7668270e-04 -1.8287926e-04 ... -4.2250078e-05\n",
      "  1.3593781e-04 -1.7859851e-04]\n",
      "[-8.9435038e-05 -8.8227178e-05  2.4653197e-04 ... -5.6219349e-05\n",
      " -9.8073215e-05 -1.6410544e-04]\n",
      "[ 1.9166659e-04 -6.6175882e-05 -1.9282057e-04 ...  1.3678898e-06\n",
      "  2.1823513e-04 -9.4872528e-05]\n",
      "[ 1.1651272e-04  2.0415015e-05  1.8744488e-04 ... -2.4325002e-04\n",
      "  1.2769386e-04 -2.3100227e-04]\n"
     ]
    }
   ],
   "source": [
    "matrix = People_Daily_2_Vec.wv.vectors\n",
    "\n",
    "for i in range(10):\n",
    "    print (matrix[i])\n",
    "\n",
    "matrix_2D = tsne.fit_transform(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_2D = tsne.fit_transform(matrix)\n",
    "\n",
    "points = pd.DataFrame(\n",
    "[\n",
    "    (word, coords[0], coords[1])\n",
    "    for word, coords in [\n",
    "        (word, matrix_2D[])\n",
    "    ]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
